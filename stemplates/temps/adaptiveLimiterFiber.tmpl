package manager

import (
	"math"
	"sync/atomic"
	"time"

	"github.com/gofiber/fiber/v3"
)

// Static rejection message to avoid map allocations and JSON encoding overhead
var (
	errLimitReachedBody = []byte(`{"error":"server capacity reached","retry_after":"1s"}`)
)

type AdaptiveLimiter struct {
	// Padded fields prevent "False Sharing" on high-core CPUs (like your 24-process setup)
	inFlight     atomic.Int64
	_            [56]byte // Padding to fill a 64-byte cache line
	limit        atomic.Int64
	_            [56]byte
	requestCount atomic.Int64
	_            [56]byte

	minLimit    int64
	maxLimit    int64
	targetP99   float64
	smoothedLat uint64 // Stored as uint64 bits for atomic float updates
	alpha       float64
	moving      bool
}

func NewAdaptiveLimiter(min, max int64, targetP99 time.Duration, moving bool) *AdaptiveLimiter {
	l := &AdaptiveLimiter{
		minLimit:  min,
		maxLimit:  max,
		targetP99: float64(targetP99.Nanoseconds()),
		alpha:     0.2,
		moving:    moving,
	}
	l.limit.Store(min + (max-min)/2)
	atomic.StoreUint64(&l.smoothedLat, math.Float64bits(float64(targetP99.Nanoseconds())))
	return l
}

func (l *AdaptiveLimiter) Handle(c fiber.Ctx) error {
	// 1. Atomic Increment of In-Flight requests
	currentInFlight := l.inFlight.Add(1)
	defer l.inFlight.Add(-1)

	currentLimit := l.limit.Load()

	// 2. Load Shedding (Zero-Allocation Rejection)
	if currentInFlight > currentLimit {
		c.Status(fiber.StatusTooManyRequests)
		c.Set(fiber.HeaderContentType, fiber.MIMEApplicationJSONCharsetUTF8)
		return c.Send(errLimitReachedBody)
	}

	start := time.Now()
	err := c.Next()
	duration := time.Since(start)

	// 3. Adaptive Adjustment (Sample every 1000 requests to keep hot path fast)
	if l.requestCount.Add(1)%1000 == 0 {
		if l.moving {
			l.adjustMoving(duration)
		} else {
			l.adjust(duration)
		}
	}

	return err
}

// adjust uses a simple additive increase/multiplicative decrease
func (l *AdaptiveLimiter) adjust(lastLatency time.Duration) {
	currLimit := l.limit.Load()
	lat := float64(lastLatency.Nanoseconds())

	if lat <= l.targetP99 && currLimit < l.maxLimit {
		l.limit.Add(1)
	} else if lat > (l.targetP99*2) && currLimit > l.minLimit {
		// Drop by 20% on latency spikes
		decrease := currLimit / 5
		if decrease < 1 {
			decrease = 1
		}
		l.limit.Add(-decrease)
	}
}

// adjustMoving uses a lock-free Exponential Weighted Moving Average (EWMA)
func (l *AdaptiveLimiter) adjustMoving(lastLatency time.Duration) {
	newLat := float64(lastLatency.Nanoseconds())
	var currentMovingAvg float64

	// Lock-free update of the float64 using CAS (Compare-And-Swap)
	for {
		oldBits := atomic.LoadUint64(&l.smoothedLat)
		oldLat := math.Float64frombits(oldBits)

		newSmoothed := (l.alpha * newLat) + ((1 - l.alpha) * oldLat)
		newBits := math.Float64bits(newSmoothed)

		if atomic.CompareAndSwapUint64(&l.smoothedLat, oldBits, newBits) {
			currentMovingAvg = newSmoothed
			break
		}
	}

	currLimit := l.limit.Load()

	if currentMovingAvg <= l.targetP99 && currLimit < l.maxLimit {
		l.limit.Add(1)
	} else if currentMovingAvg > (l.targetP99*1.5) && currLimit > l.minLimit {
		decrease := currLimit / 5
		if decrease < 1 {
			decrease = 1
		}
		l.limit.Add(-decrease)
	}
}
